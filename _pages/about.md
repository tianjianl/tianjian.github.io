---
layout: about
title: about
permalink: /
subtitle: <a href='https://www.clsp.jhu.edu/'>Center for Language and Speech Processing</a>,  <a href='https://www.jhu.edu/'>Johns Hopkins University</a>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular

news: true  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

My name is Tianjian Li. I have received my B.A. in Mathematics and Computer Science from New York University. I am now pursuing a M.S. degree at Johns Hopkins University. I am currently doing research on multilingual language models and machine translation under the supervision of [Kenton Murray](https://kentonmurray.com/) and [Philipp Koehn](https://www.cs.jhu.edu/~phi/).

My research interests mainly lies in the intersection of deep learning and natural language processing, with a focus on NLP under multilingual settings. I aim to answer these two questions:

- How to measure parameter and data utility through the lens of machine translation?
- How to build practical multilingual NLP systems with [limited training data](https://arxiv.org/abs/2305.17325) and computational resources?

I prefer solutions that possess **simplicity**, exhibit strong **generalizability**, and grounded in **theory**.

Previously, I was working as a research intern at the Knowledge Engineering Group of Tsinghua University, under the supervision of [Jie Tang](http://keg.cs.tsinghua.edu.cn/jietang/). I have built a 1B decoder-only multilingual language model mGLM, which excels in multilingual generation tasks due to its autoregressive denoising pre-train objective. mGLM is now [open sourced](https://github.com/THUDM/Multilingual-GLM).

If you have anything to share with me, please feel free to contact me through my email: tli104@jhu.edu

